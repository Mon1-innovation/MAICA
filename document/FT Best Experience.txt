I'm sorry for not offering an English ver of this document but it's just too much work for me.
If you want to read in English, use a translator.

说实话, 我懒得写微调的最佳实践了, 我真不觉得会有人用得上这东西.
我把我自己的训练脚本贴在这里吧. 使用环境是ms-swift 3.8.
swift, TE之类的东西都是我自己改过的, 不过如果你的设备环境没有这么极端, 应该能少很多麻烦才对.
如果你真的需要更具体的帮助之类的, 可以直接联系我.

multiply_numbers() {
    local input="$1"
    local multiplier="$2"

    echo "$input" | sed -E "s/#([0-9]+)/#\$((\1 * $multiplier))/g" | bash -c 'eval echo "$(cat)"'
}
# 10ep
echo 'nproc nums?'
read npn
echo 'resume from path, leave empty for using model directly:'
read rfc
echo 'training epoches, leave empty for 1:'
read nte
echo 'mixed epoches, leave empty for 1:'
read mte
#ctdp='dataset/moni_dataset.jsonl dataset/moni_dataset.jsonl dataset/moni_dataset.jsonl dataset/moni_dataset.jsonl dataset/moni_dataset_en.jsonl dataset/moni_dataset_en.jsonl dataset/covid_new.jsonl dataset/covid_new.jsonl dataset/covid_new.jsonl dataset/covid_new.jsonl dataset/covid_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl dataset/ds_new.jsonl HF::roleplay-base#2500 iic/MSAgent-MultiRole#1000 AI-ModelScope/wikipedia-cn-20230720-filtered#500 AI-ModelScope/ruozhiba#500 modelscope/chinese-poetry-collection#1000 AI-ModelScope/webnovel_cn#500'
ds1='dataset/moni_dataset.jsonl#1598'
ds2='dataset/moni_dataset_en.jsonl#1131'
ds3='dataset/covid_new.jsonl#50'
ds4='dataset/ds_new.jsonl#50'
ds5='AI-ModelScope/deepctrl-sft-data:default#1900 AI-ModelScope/deepctrl-sft-data:en#1150 AI-ModelScope/ruozhiba:post-annual#650'
ds6='dataset/daa/daa_mix1.jsonl#676'
ds7='dataset/daa/daa_norm2.jsonl#144'
ds8='dataset/daa/daa_prcs2.jsonl#296'
ds9='dataset/daa/daa_norm3.jsonl#205'
ds10='dataset/daa/daa_prcs3.jsonl#400'
ds11='dataset/daa/daa_prcs3_en.jsonl#68'
ctdp=$ds1' '$ds2' '$ds3' '$ds4' '$ds5' '$ds6' '$ds7' '$ds8' '$ds9' '$ds10' '$ds11
ctdp=$(multiply_numbers "$ctdp" $mte)
echo $ctdp
if [ ! -n "$npn" ]
then
    npn=12
fi
if [ ! -n "$rfc" ]
then
    rfc=null
    ft=true
    # rfc='/root/swift/erepo/Megatron-LM/Qwen3-235B-A22B-Instruct-2507-mcore'
    # rfc='/root/swift/erepo/Megatron-LM/Qwen3-30B-A3B-mcore'
else
    rfc="$rfc"
    ft=false
fi
if [ ! -n "$nte" ]
then
    nte=1
fi
if [ ! -n "$mte" ]
then
    mte=1
fi
nproc_per_node=$npn
export HTTPS_PROXY=192.168.3.254:7890
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
# PYTHONPATH=/root/swift \
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11
# export CUDA_DEVICE_ORDER=PCI_BUS_ID
export NPROC_PER_NODE=$npn
# export ACCELERATE_BYPASS_DEVICE_MAP=true
# export OMP_NUM_THREADS=4
# export CUDA_LAUNCH_BLOCKING=1
# NVTE_DEBUG=1 NVTE_DEBUG_LEVEL=2 \
export NVTE_CUDA_INCLUDE_DIR=/usr/local/cuda
export MEGATRON_LM_PATH=/root/swift/erepo/Megatron-LM

#model Qwen/Qwen3-235B-A22B-Instruct-2507
params=(
    # --model Qwen/Qwen3-235B-A22B-Instruct-2507
    --model_type 'qwen3_nothinking'
    --template 'qwen3_nothinking'
    --load /root/swift/erepo/Megatron-LM/Qwen3-235B-A22B-Instruct-2507-mcore
    --dataset $ctdp
    --optimizer_cpu_offload true
    --use_precision_aware_optimizer true
    --train_type lora
    --lora_rank 36
    --lora_alpha 64
    --custom_dataset_info /root/swift/dataset/ds_info.json
    --val_dataset '/root/swift/dataset/eval.jsonl'
    --target_modules all-linear
    --split_dataset_ratio 0.0
    --expert_model_parallel_size 1
    --pipeline_model_parallel_size 12
    --moe_grouped_gemm true
    --moe_router_dtype 'fp64'
    --moe_router_topk 12
    --moe_permute_fusion true
    --moe_shared_expert_overlap true
    --moe_aux_loss_coeff 1e-3
    --micro_batch_size 2
    --global_batch_size 16
    --recompute_granularity full
    --recompute_method uniform
    --recompute_num_layers 1
    --max_epochs $nte
    --finetune $ft
    --cross_entropy_loss_fusion true
    --lr 1e-4
    --lr_warmup_fraction 0.05
    --min_lr 1e-5
    --save megatron_output/Qwen3-235B-A22B-Instruct-2507
    --eval_interval 50
    --save_interval 100
    --max_length 1600
    --num_workers 12
    --dataset_num_proc 12
    --no_save_optim true
    # --no_load_optim true
    --no_save_rng true
    # --no_load_rng true
    --sequence_parallel false
    --attention_backend flash
    --pipeline_model_parallel_layout "Et*16|t*17|t*17|(t*5|)*8,t*4L"
    # --adapter_load $rfc
    # --padding_free false
    # --mlp_padding_free true
    # --pipeline_model_parallel_layout "Et*9|t*9|(t*3|)*9,t*2L"
    # --load_from_cache_file false
)
megatron sft "${params[@]}"
